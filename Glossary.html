<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Glossary</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <!-- CSS added by filter 'toc-css.lua' for TOC hovering to the side -->
  <!-- TODO: try doing sometthing in px instead of cm to make it more device independent -->
  <style>
  body {
    padding-left: 1.5cm;
    padding-right: 1cm;
    transition: 0.5s;
  }
  nav {
    width: 1cm;
    margin-left: -1.5cm;
    font-size: smaller;
    color: grey;
    transition: 0.5s;
    float: left;
    position: fixed;
    top: 0;
    bottom: 0;
    white-space: nowrap;
    overflow: hidden;
    overflow-y: scroll;
    transition: 0.5s;
    z-index: 10;
  }
  nav::-webkit-scrollbar {
    display: none;
  }
  nav a, nav a:visited {
    color: grey;
  }
  nav h2:before {
    content: "‚â° ";
    font-size: 150%;
  }
  nav h2:after {
    content: " ‚óÇ";
  }
  nav li {
    margin-left: 1em;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
  }
  nav li > a:not(:only-child):before {
    content: "‚ñ∏ ";
  }
  nav li > a:only-child {
    margin-left: 0.75em;
  }
  nav li li {
    margin-left: 1em;
  }
  nav li li li {
    margin-left: 0.5em;
    font-size: smaller;
  }
  nav ul li ul  {
    visibility: hidden;
    display: none;
    margin-top: 0.2em;
    margin-bottom: 0.2em;
    transition: 0.5s;
  }
  .paddingleft {
    padding-left: 9cm;
    transition: 0.5s;
  }
  .navside {
    width: 7cm;
    margin-left: -8.5cm;
    padding-right: 1cm;
    transition: 0.5s;
  }
  .navside h2:after {
    content: " ‚ñ∏ ";
  }
  .navshown {
    width: 50%;
    transition: 0.5s;
    background-color: rgba(255, 255, 255, 0.95);
  }
  .subShow > ul {
    visibility: visible;
    display: block;
    transition: 0.5s;
    margin-left: -1em;
  }
  .subShow > a:not(:only-child):before {
    content: " ‚ñæ ";
  }

  .toc-invisible {
    visibility: hidden;
  }
  .navside > .toc-invisible {
    visibility: visible;
  }
  .navshown > .toc-invisible {
    visibility: visible;
  }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Glossary</h1>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#asterism-">Asterism (‚ÅÇ)</a></li>
<li><a href="#attg">ATTG</a></li>
<li><a href="#attributes">Attributes</a></li>
<li><a href="#authors-notes-an">Author's Notes (A/N)</a></li>
<li><a href="#banned-tokens">Banned Tokens</a></li>
<li><a href="#brackets">Brackets</a></li>
<li><a href="#branch">Branch</a></li>
<li><a href="#calliope-model">Calliope (model)</a></li>
<li><a href="#canvas">Canvas</a></li>
<li><a href="#cascading-activation">Cascading Activation</a></li>
<li><a href="#caveman-format">Caveman Format</a></li>
<li><a href="#clio">Clio</a></li>
<li><a href="#context">Context</a></li>
<li><a href="#context-window">Context Window</a></li>
<li><a href="#context-viewer">Context Viewer</a></li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#dinkus-">Dinkus (***)</a></li>
<li><a href="#end-of-sampling-eos">End of Sampling (EOS)</a></li>
<li><a href="#end-of-text-token-endoftext">End of Text Token (&lt;|endoftext|&gt;)</a></li>
<li><a href="#entry-or-lorebook-entry">Entry (or Lorebook Entry)</a></li>
<li><a href="#ephemeral-context">Ephemeral Context</a></li>
<li><a href="#euterpe-model">Euterpe (Model)</a></li>
<li><a href="#finetune">Finetune</a></li>
<li><a href="#flatten">Flatten</a></li>
<li><a href="#format-or-lorebook-format">Format (or Lorebook Format)</a></li>
<li><a href="#generation-or-ai-generation">Generation (or AI Generation)</a></li>
<li><a href="#generative-pre-trained-transformer-gpt">Generative Pre-trained Transformer (GPT)</a></li>
<li><a href="#guidance">Guidance</a></li>
<li><a href="#image-resolution">Image Resolution</a></li>
<li><a href="#image-to-image-img2img">Image To Image (Img2Img)</a></li>
<li><a href="#inline-generation">Inline Generation</a></li>
<li><a href="#inpainting">Inpainting</a></li>
<li><a href="#lore-generator">Lore Generator</a></li>
<li><a href="#krake-model">Krake (Model)</a></li>
<li><a href="#lorebook">Lorebook</a></li>
<li><a href="#lorebook-keys">Lorebook Keys</a></li>
<li><a href="#loss-or-training-loss">Loss (or Training Loss)</a></li>
<li><a href="#loss-graph">Loss Graph</a></li>
<li><a href="#memory">Memory</a></li>
<li><a href="#model">Model</a></li>
<li><a href="#module-or-ai-module">Module (or AI Module)</a></li>
<li><a href="#module-training">Module Training</a></li>
<li><a href="#nai-diffusion-anime-curated">NAI Diffusion Anime Curated</a></li>
<li><a href="#nai-diffusion-anime-full">NAI Diffusion Anime Full</a></li>
<li><a href="#nai-diffusion-furry">NAI Diffusion Furry</a></li>
<li><a href="#noise">Noise</a></li>
<li><a href="#nucleus-sampling-top-p">Nucleus Sampling (Top-P)</a></li>
<li><a href="#order-of-insertion">Order of Insertion</a></li>
<li><a href="#output">Output</a></li>
<li><a href="#perspective-or-pov">Perspective (or POV)</a></li>
<li><a href="#phrase-bias">Phrase Bias</a></li>
<li><a href="#placeholder">Placeholder</a></li>
<li><a href="#preamble">Preamble</a></li>
<li><a href="#prefix">Prefix</a></li>
<li><a href="#preset-or-generation-preset">Preset (or Generation Preset)</a></li>
<li><a href="#prompt">Prompt</a></li>
<li><a href="#prose">Prose</a></li>
<li><a href="#quality-tags">Quality Tags</a></li>
<li><a href="#randomness-or-temperature">Randomness (or Temperature)</a></li>
<li><a href="#redo-tree">Redo Tree</a></li>
<li><a href="#repetition-penalty">Repetition Penalty</a></li>
<li><a href="#reserved-tokens">Reserved Tokens</a></li>
<li><a href="#retry">Retry</a></li>
<li><a href="#sampling">Sampling</a></li>
<li><a href="#scale">Scale</a></li>
<li><a href="#scenario">Scenario</a></li>
<li><a href="#search-range">Search Range</a></li>
<li><a href="#seed">Seed</a></li>
<li><a href="#separator-----">Separator (----)</a></li>
<li><a href="#sigurd-model">Sigurd (model)</a></li>
<li><a href="#stable-diffusion">Stable Diffusion</a></li>
<li><a href="#step">Step</a></li>
<li><a href="#storage">Storage</a></li>
<li><a href="#story">Story</a></li>
<li><a href="#story-tag">Story Tag</a></li>
<li><a href="#stream-responses">Stream Responses</a></li>
<li><a href="#subcontext">Subcontext</a></li>
<li><a href="#suffix">Suffix</a></li>
<li><a href="#tags">Tags</a></li>
<li><a href="#tail-free-sampling-tfs">Tail-Free Sampling (TFS)</a></li>
<li><a href="#text-adventure-module">Text Adventure Module</a></li>
<li><a href="#theme">Theme</a></li>
<li><a href="#tier">Tier</a></li>
<li><a href="#token">Token</a></li>
<li><a href="#token-budget">Token Budget</a></li>
<li><a href="#token-id">Token ID</a></li>
<li><a href="#tokenization">Tokenization</a></li>
<li><a href="#tokensafe">Tokensafe</a></li>
<li><a href="#top-a">Top-A</a></li>
<li><a href="#top-k">Top-K</a></li>
<li><a href="#top-kek">Top-Kek</a></li>
<li><a href="#top-p">Top-P</a></li>
<li><a href="#typical-sampling">Typical Sampling</a></li>
<li><a href="#train-a-model">Train (a Model)</a></li>
<li><a href="#trim">Trim</a></li>
<li><a href="#undesired-content">Undesired Content</a></li>
</ul>
</nav>
<p>This page aims to define terms that are often used by the community and may not be immediately accessible to newcomers.</p>
<h2 id="asterism-">Asterism (‚ÅÇ)</h2>
<p><em>Typography</em></p>
<p>A group of three asterisks brought together as a single symbol. Usually used to separate stories in the finetune, making it a powerful separator. Similar, but not identical to a Dinkus, which is weaker.</p>
<h2 id="attg">ATTG</h2>
<p><em>Technique</em></p>
<p>ATTG stands for the <code>[ Author: ‚Ä¶; Title: ‚Ä¶; Tags: ‚Ä¶; Genre: ‚Ä¶ ]</code> metadata, which is present in the dataset. This row can be inserted at the top of the Context to heavily influence the AI output. The first two categories are usually omitted as they're less useful than Tags and Genre.</p>
<h2 id="attributes">Attributes</h2>
<p><em>Technique</em></p>
<p>Attributes is a technique where various data --- such as characters or locations --- is formatted similarly to database entries or wiki-style data blocks. Due to these being common in both the Base and the Dataset, the Attributes can be as efficient as prose --- and often easier to write.</p>
<h2 id="authors-notes-an">Author's Notes (A/N)</h2>
<p><em>Text Injection, Feature</em></p>
<p>A type of <a href="Context.html">Text Injection</a> that is usually situated three newlines from the bottom of the context. It is much stronger than Memory and can often be disruptive, especially if the inserted text doesn't fit the output seamlessly. The typical side-effects include looping and degradation of the output quality. Use with caution.</p>
<h2 id="banned-tokens">Banned Tokens</h2>
<p><em>Generation, Feature</em></p>
<p>A token the AI is not allowed to generate. Instead, the AI will use the next most likely token.</p>
<h2 id="brackets">Brackets</h2>
<p><em>Typography</em></p>
<p>Shorthand for <em>Square Brackets</em>, which are these symbols: [ ]. They are part of the finetune data, and used to contain metadata about the text files. They are traditionally used in <a href="Context.html">Injected Text</a> to separate it from story text.</p>
<h2 id="branch">Branch</h2>
<p><em>Generation, Writing</em></p>
<p>A version of the story. When you redo, or undo then edit, you create a new branch. Think of it like time travel.</p>
<h2 id="calliope-model">Calliope (model)</h2>
<p><em>AI</em></p>
<p>A finetuned version of the GPT-NEO 2.7B model. It is inferior in creative performance to Sigurd, but very lightweight and can run on cheaper, slower hardware. It was used by NovelAI during the Pre-Alpha and Alpha stages.</p>
<h2 id="canvas">Canvas</h2>
<p><em>Image Generation</em></p>
<p>The area taken by the image. You can freely edit it directly in NovelAI, or import it in the image editing tool of your choice.</p>
<h2 id="cascading-activation">Cascading Activation</h2>
<p><em>Injected Text, Generation</em></p>
<p>A flag that tells a Lorebook entry to check for other text injections (Memory, Author's Note) for its keys, rather than only the story text.</p>
<h2 id="caveman-format">Caveman Format</h2>
<p><em>Writing, Technique</em></p>
<p>Writing while removing most grammatical words, and focusing only on words that carry semantic meaning. As an example, <code>John usually wears a bright blue coat which he likes dearly</code> would become <code>john wear bright blue coat, john like coat</code>. It is used for lorebook entries to reinforce connections between subjects and objects and reduce token use at the expense of writing quality. Caveman is mostly obsolete in advanced models.</p>
<h2 id="clio">Clio</h2>
<p><em>AI</em></p>
<p>A in-house model trained by NovelAI. It boasts a much larger context and higher performance than Krake with only 3 Billion parameters.</p>
<h2 id="context">Context</h2>
<p><em>Injected Text, Generation</em></p>
<p>All the text that the AI has in its memory before it attempts to generate more text. This is all the activated injected text, plus all the story text that can fit inside the context window.</p>
<h2 id="context-window">Context Window</h2>
<p><em>AI, Limit</em></p>
<p>GPT-J and GPT-NEO both share a limit of 2048 tokens, or ~9000 Latin Alphabet characters used as a person would need to communicate in a natural manner. This is the maximum memory that the AI can use. On Tablet tier, the window is 1024 Tokens.</p>
<p>For Clio, this limit is 8192 Context for Opus, which is around 35000 characters.</p>
<h2 id="context-viewer">Context Viewer</h2>
<p><em>Injected Text, Feature</em></p>
<p>A tool that lets you visualize all text injections, and what the context window contains. This can help diagnose generation problems due to poor text injection settings.</p>
<h2 id="dataset">Dataset</h2>
<p><em>AI</em></p>
<p>A batch of text files, used for fine tuning or module training. It must be presented in a specific format for best results.</p>
<h2 id="dinkus-">Dinkus (***)</h2>
<p><em>Typography</em></p>
<p>A set of three asterisks in a row. Often used to separate large sections. Breaks the current event sequence before resuming generation. Can be used for things like short time skips, depending on how the previous paragraph ended. Since Euterpe, Dinkus has been used mainly for prose. For separating data, see <code>Separator (----)</code>.</p>
<h2 id="end-of-sampling-eos">End of Sampling (EOS)</h2>
<p><em>AI, Generation</em></p>
<p>A marker used to tell the AI to stop generating. Generally, it is an uncommon symbol that would not appear unless the user 'trains' the AI to use it regularly. Useful for generating text similar to chat logs and other unorthodox forms, or simply to stop on sentence ending markers.</p>
<h2 id="end-of-text-token-endoftext">End of Text Token (&lt;|endoftext|&gt;)</h2>
<p><em>AI</em></p>
<p>A token that was used to designate the end of a text file, so that the training routine of the AI can proceed to the next file. It can rarely appear during generation, this is simply an artifact and can be removed.</p>
<h2 id="entry-or-lorebook-entry">Entry (or Lorebook Entry)</h2>
<p><em>Text Injection</em></p>
<p>An entry for an element of your story, as part of its lorebook. It has text data, insertion settings, and keys that will activate it if present.</p>
<h2 id="ephemeral-context">Ephemeral Context</h2>
<p><em>Text Injection, Feature</em></p>
<p>A form of Text Injection that is only active for a number of actions, after which it disappears. Has a very specific syntax that no other feature uses.</p>
<h2 id="euterpe-model">Euterpe (Model)</h2>
<p><em>AI</em></p>
<p>A finetune of the Fairseq GPT-13B model. It is superior in creative performance to Sigurd, but more expensive to train and run. It is used by NovelAI during the Beta stage.</p>
<h2 id="finetune">Finetune</h2>
<p><em>AI</em></p>
<p>If you consider the AI as a bunch of knobs and sliders, this is the act of adjusting them in order to make the AI's output more "fitting" for your purposes. This is done on expensive, powerful machines or servers and not really something you do yourself.</p>
<h2 id="flatten">Flatten</h2>
<p><em>Feature</em></p>
<p>An aggressive form of trimming. Eliminates all branches, which leaves the entire story as one solid block of text.</p>
<h2 id="format-or-lorebook-format">Format (or Lorebook Format)</h2>
<p><em>Writing</em></p>
<p>A way to present the information to the AI. It can range from writing things as you would in a normal essay, to using formats similar to code, to many other different types.</p>
<h2 id="generation-or-ai-generation">Generation (or AI Generation)</h2>
<p><em>Generation</em></p>
<p>After the AI receives the context window's data, it tries to continue the story from there. The text you receive from the AI is the Generation.</p>
<h2 id="generative-pre-trained-transformer-gpt">Generative Pre-trained Transformer (GPT)</h2>
<p><em>AI</em></p>
<p>A neural network that takes the form of a large amount of <a href="https://en.wikipedia.org/wiki/Vector_space">vector-space equations</a>. Every number is a token, which is a text fragment that appeared in the AI's training data. The fragments, and their relationships, are analyzed, and a network of "relationships" between all these fragments are created. The goal is to create a network which can replicate human language in order to convincingly generate human-like text.</p>
<p>As it is pre-trained, it does not learn from its input. Only what it has been trained with. Training is an extremely costly operation that requires rare, expensive, and difficult to set up hardware, making it a costly service. These models also require powerful Graphical Processing Units, especially units equipped with Tensor Cores, which are ideal for vector space math (like raytracing!) These GPUs are expensive and use a lot of power, which also makes <em>running</em> a GPT model expensive.</p>
<h2 id="guidance">Guidance</h2>
<p><em>Image Generation</em></p>
<p>See <a href="Glossary.html#scale">Scale</a>.</p>
<h2 id="image-resolution">Image Resolution</h2>
<p><em>Image Generation</em></p>
<p>The total surface of your image, defined by its Width, multiplied by its Height. NovelAI can generate images up to ~3 Megapixels in resolution (1024x1024).</p>
<h2 id="image-to-image-img2img">Image To Image (Img2Img)</h2>
<p><em>Image Generation</em></p>
<p>Using an image as a base for the AI to modify. The prompt still applies, allowing you to change, add, or remove elements from the original picture.</p>
<h2 id="inline-generation">Inline Generation</h2>
<p><em>Feature, Generation</em></p>
<p>An action performed by pressing Ctrl + Shift + Enter or ‚åò + Shift + Enter, with the text cursor located in the main text box. This will generate a response where the text cursor is, rather than at the end. Content after the text cursor is ignored by the AI.</p>
<h2 id="inpainting">Inpainting</h2>
<p><em>Image Generation</em></p>
<p>Selecting an area of an image and prompting the Image Generation AI to redraw only this area, alternatively, providing an image with a missing chunk on its inside, and telling the Image Generation AI to complete the missing chunk.</p>
<h2 id="lore-generator">Lore Generator</h2>
<p><em>Feature, Generation</em></p>
<p>A feature of the Lorebook that uses the current module (and if desired, the Memory, Author's Note, and other lorebook entries) to generate lore for concepts, objects, people, factions, and much more.</p>
<h2 id="krake-model">Krake (Model)</h2>
<p><em>AI</em></p>
<p>A finetune of EleutherAI's 20B GPT-NeoX model. It offers similar geenral metrics to Euterpe but has a different set of strengths, namely in terms of real world knowledge, and style-matching abilities.</p>
<h2 id="lorebook">Lorebook</h2>
<p><em>Text Injection, User Content</em></p>
<p>A database of entries, triggered by keys. If an entry's key is found, then the text of the entry is injected into the context, based on its settings. Works identically to other text injections, just with more fine control over where it ends up.</p>
<h2 id="lorebook-keys">Lorebook Keys</h2>
<p><em>Text Injection</em></p>
<p>A text "trigger" which activates a Lorebook entry if found in the text. Can be anything, from a name, to a symbol.</p>
<h2 id="loss-or-training-loss">Loss (or Training Loss)</h2>
<p><em>AI</em></p>
<p>Loss is a term used in Machine Learning to determine if the AI's output is close to what was expected. When training Modules, lower loss means that it deviates less from the dataset, whereas high loss usually mean that the data was improperly cleaned, or it could just mean that the data is different from what the AI has learned before.</p>
<h2 id="loss-graph">Loss Graph</h2>
<p><em>AI</em></p>
<p>A graphical representation of the module's loss after every step of the AI's reading and training. A good 'level' for loss is very subjective, generally, you want to avoid the curve following a path that goes too low or too high, as both have their own issues.</p>
<h2 id="memory">Memory</h2>
<p><em>Text Injection</em></p>
<p>A piece of text that is inserted near the top of the story. Traditionally used to keep track of broad context elements and current events to help the AI stay on track. Works the same as Author's Note otherwise.</p>
<h2 id="model">Model</h2>
<p><em>AI</em></p>
<p>A code base which constitutes the structure on which the neural network is built. GPT-NEO and GPT-J are both model codebases. GPT-J <strong>6B</strong>, however, is a <strong>weight</strong>, which is a pre-trained model that possesses 6 <strong>billion</strong> parameters. Parameters are the number of connections between existing tokens.</p>
<p>Image generation uses different models, which are finetunes of Stable Diffusion, produced by Stability.AI.</p>
<h2 id="module-or-ai-module">Module (or AI Module)</h2>
<p><em>AI, User Content</em></p>
<p>Known as a <strong>soft prompt</strong> in the research community. This is a batch of vector math that is sent directly to the AI, ignoring tokenization completely. It takes the same amount of space in the context window as 20 tokens, but is not a token by itself. Think of it as a mini finetune based on a specific group of texts that does not require re-training the AI as a whole, but still changes what the AI will produce.</p>
<h2 id="module-training">Module Training</h2>
<p><em>AI</em></p>
<p>Sending text files to NovelAI's model training program in order to produce a module trained on their content. Works similarly to fine-tuning a model, but on a smaller scale.</p>
<h2 id="nai-diffusion-anime-curated">NAI Diffusion Anime Curated</h2>
<p><em>Image Generation, Model</em></p>
<p>A finetune of Stable Diffusion, trained on Danbooru images, all of which are tagged as Safe, ensuing no NSFW output. Generally outputs more consistent images and is better at pre-existing characters, to some degree.</p>
<h2 id="nai-diffusion-anime-full">NAI Diffusion Anime Full</h2>
<p><em>Image Generation, Model</em></p>
<p>A finetune of Stable Diffusion, trained on Danbooru images, which is capable of NSFW output, as it uses all types of images. Outputs greater variety for characters.</p>
<h2 id="nai-diffusion-furry">NAI Diffusion Furry</h2>
<p><em>Image Generation, Model</em></p>
<p>A finetune of Stable Diffusion, based on images from various sources with E621 tagging. The main focus is on anthropomorphic characters, but the model has a broad coverage.</p>
<h2 id="noise">Noise</h2>
<p><em>Image Generation</em> Noise is the base for all Stable Diffusion images (hence the name of diffusion). The noise pattern is refined into an image after every step.</p>
<p>As a parameter, Noise <em>adds</em> noise to the existing image in order to allow for greater deviation from the base.</p>
<h2 id="nucleus-sampling-top-p">Nucleus Sampling (Top-P)</h2>
<p><em>AI, Sampling</em></p>
<p>Sampling which selects a set of tokens, based on the percentile chance of them appearing. Starting from the most likely, it adds up probabilities until it hits its setting. Sampling then ends.</p>
<h2 id="order-of-insertion">Order of Insertion</h2>
<p><em>Text Injection</em></p>
<p>A setting which determines the order that text injections are performed. Think of it as a stack: The highest number goes first.</p>
<h2 id="output">Output</h2>
<p><em>AI</em></p>
<p>Synonymous with AI Response or Generation. Effectively, what the AI makes based on what you give it.</p>
<h2 id="perspective-or-pov">Perspective (or POV)</h2>
<p><em>Writing</em></p>
<p>Whether or not the narration is done in first (I/me), second (You), or third person(he/she/it).</p>
<h2 id="phrase-bias">Phrase Bias</h2>
<p><em>Feature, Generation</em></p>
<p>A feature which influences the AI to increase or reduce the likelihood of some tokens or phrases appearing. Good for enforcing consistency or reducing things you don't want without outright bans.</p>
<h2 id="placeholder">Placeholder</h2>
<p><em>Feature</em></p>
<p>A text element that must be filled by a scenario's user upon import.</p>
<h2 id="preamble">Preamble</h2>
<p><em>Generation</em></p>
<p>The Preamble is a sequence of tokens inserted in the context window before generation. It tells the AI "this is the start of something new", which can help reducing the feeling of "being in the middle of something" and sticking to your prompt a little more closely. The Preamble used depends on the model.</p>
<h2 id="prefix">Prefix</h2>
<p><em>Text Injection</em></p>
<p>A small piece of text appended to the beginning of a Text Injection when it is placed in the context window. Helps with separating it from other entries if you use brackets or newlines.</p>
<h2 id="preset-or-generation-preset">Preset (or Generation Preset)</h2>
<p><em>AI, User Content</em></p>
<p>A batch of settings, such as sampling and randomness, that can be saved and shared to other users.</p>
<h2 id="prompt">Prompt</h2>
<p><em>Writing</em></p>
<p>The first piece of story text that you provide to the AI, before any generation has been performed.</p>
<p><em>Image Generation</em></p>
<p>The instructions given to the Diffusion model to direct its output.</p>
<h2 id="prose">Prose</h2>
<p><em>Writing</em></p>
<p>Writing like you speak. Prose is the opposite of <strong>Verse</strong> which follows a structure, meter, or both.</p>
<p><em>Image Generation</em></p>
<p>A prompt written in natural language, as opposed to using tags (although both can be mixed.)</p>
<h2 id="quality-tags">Quality Tags</h2>
<p><em>Image Generation</em></p>
<p>Tags that were applied to images used for the training of NovelAI Diffusion's models. They are based on a percentile score, with the top scoring images being ranked as "masterpiece", all the way through "best quality", "high quality", "normal quality", "low quality" and "worst quality". This is used to discern "good" elements from "bad" ones.</p>
<h2 id="randomness-or-temperature">Randomness (or Temperature)</h2>
<p><em>AI</em></p>
<p>A setting that adjusts the likelihood of the tokens available to the AI after sampling, so that rare tokens actually have a chance of appearing.</p>
<h2 id="redo-tree">Redo Tree</h2>
<p><em>Feature</em></p>
<p>When you have performed one or more Retries or edits for a story step, this button with a number next to Redo will list all the Retries and edits you have done, so you can pick the one you prefer.</p>
<h2 id="repetition-penalty">Repetition Penalty</h2>
<p><em>AI</em></p>
<p>A setting that decreases a token's chance of appearing if it has already appeared in the current generation. Helps reduce repetitiveness.</p>
<h2 id="reserved-tokens">Reserved Tokens</h2>
<p><em>Text Injection</em></p>
<p>How many tokens' worth of space are saved for the current Text Injection. Highest priority reserves first.</p>
<h2 id="retry">Retry</h2>
<p><em>Feature</em></p>
<p>Deleting the previous AI generation and asking the AI to send a new one. If you made any edits, this is just like clicking Send.</p>
<h2 id="sampling">Sampling</h2>
<p><em>AI, Sampling</em></p>
<p>Selecting a limited pool of tokens by doing some math on their probabilities. This helps focus the AI and make Randomness more useful by spreading it over less possible tokens.</p>
<p><em>Image Generation</em> The sampler is an "interpreter" of sorts, which handles how the prompt is processed. The default, k_euler_ancestral, offers greater quality at lower steps, and more per-step variability, ensuing a lot of potential without using a lot of steps per image.</p>
<h2 id="scale">Scale</h2>
<p><em>Image Generation</em></p>
<p>A setting used by Stable Diffusion to adjust how the image is generated. Generally, higher scale leads to stronger contrast, and "sharper" images, whereas low scale is "softer" and more "painterly".</p>
<p>It is a floating point number, and can be fine-tuned down to several decimal places.</p>
<h2 id="scenario">Scenario</h2>
<p><em>User Content</em></p>
<p>A story that was exported for sharing. It can have placeholders: elements that must be filled in when imported by the user, such as names and so on. This allows for easier personalization than a normal prompt. It can also contain a lorebook and generation settings.</p>
<h2 id="search-range">Search Range</h2>
<p><em>Feature</em></p>
<p>A <strong>lorebook</strong> setting that decides how deep in the story text it will go looking for keys before it stops. Helpful if you mention a lot of things but only want to focus on very recent elements.</p>
<h2 id="seed">Seed</h2>
<p><em>Image Generation</em></p>
<p>A 32-bit integer which serves to define the base noise pattern, which is then refined into an image. Using the same seed will result in similar images (to some degree). Exactly identical settings and prompt are required for an identical output.</p>
<h2 id="separator-----">Separator (----)</h2>
<p><em>Typography</em></p>
<p>A set of four hyphens. In the Dataset, this is used to separate the individual data entries for things like Attributes-style character data. As such, it should be used instead of Dinkus (***) if you want a module or scenario to output non-prose. Useful for Generators and other similar uses.</p>
<h2 id="sigurd-model">Sigurd (model)</h2>
<p><em>AI</em></p>
<p>A finetuned version of the GPT-J 6B model. It is superior in creative performance to Calliope, but more expensive to train and run. It is used by NovelAI during the Beta stage.</p>
<h2 id="stable-diffusion">Stable Diffusion</h2>
<p><em>Image Generation</em></p>
<p>An image generation AI system by StabilityAI. As a diffusion model, it generates images based on noise patterns, rather than collaging pictures together. It is open source and can be downloaded and run for free with a suitable GPU. NovelAI's implementation uses additional proprietary code extensions and finetunes.</p>
<h2 id="step">Step</h2>
<p><em>Writing</em></p>
<p>Every time a generation is performed, the story advances to its next step. Edits after a generation count as another step as well.</p>
<p><em>Image Generation</em></p>
<p>One of the de-noising, image-drawing phases performed by Stable Diffusion. Steps are not linear: You cannot simply "add" another step, the entire generation has to be planned for the step amount.</p>
<h2 id="storage">Storage</h2>
<p><em>Feature</em></p>
<p>An encrypted database of your stories. Can be local, or on NovelAI's servers.</p>
<h2 id="story">Story</h2>
<p><em>Writing</em></p>
<p>The content in the text box in the middle of your NovelAI window is "the story text." Every other piece of text fed into the AI is Text Injection.</p>
<h2 id="story-tag">Story Tag</h2>
<p><em>Feature</em></p>
<p>A tag applied to a story so you can search for it easier.</p>
<h2 id="stream-responses">Stream Responses</h2>
<p><em>AI, Feature</em></p>
<p>Displays tokens as they are generated rather than dumping the entire response on you all at once, as if you could watch the AI writing it.</p>
<h2 id="subcontext">Subcontext</h2>
<p><em>Text Injection</em></p>
<p>A group of Lorebook Entries that are injected as one block with its own insertion settings.</p>
<h2 id="suffix">Suffix</h2>
<p><em>Text Injection</em></p>
<p>Identical to <em>Prefix</em>, but it is added to the end of the entry.</p>
<h2 id="tags">Tags</h2>
<p><em>Image Generation</em></p>
<p>Refers to Danbooru tags (Curated, Full), or E621 (furry), which are used for training the AI and make generating specific elements easier.</p>
<h2 id="tail-free-sampling-tfs">Tail-Free Sampling (TFS)</h2>
<p><em>AI, Sampling</em></p>
<p>A "Tail" is basically all the tokens that are <strong>really</strong> not appropriate, such as random garbage symbols or extreme outliers. Most sampling methods are very aggressive and only keep the absolute best tokens. Tail-Free attempts to only cut off the tail so that the pool of possible tokens is bigger and creative, without keeping the really bad tokens.</p>
<h2 id="text-adventure-module">Text Adventure Module</h2>
<p><em>AI, Feature</em></p>
<p>A module designed to be played with DO and SAY inputs, similar to old Text Adventure games like Zork. Uses a different interface where inputs and outputs are clearly delineated.</p>
<h2 id="theme">Theme</h2>
<p><em>Feature</em></p>
<p>A JSON file with some CSS thrown in it, which tells NovelAI's page how to look like. Allows for considerable customization of text colors, backgrounds, and many other things.</p>
<h2 id="tier">Tier</h2>
<p><em>Subscription</em></p>
<p>Tablet, Scroll and Opus are the three Tiers that you can pay for. Generally, Opus has access to features early and some extras, while Tablet has a limited context window.</p>
<h2 id="token">Token</h2>
<p><em>AI</em></p>
<p>A fragment of text that is turned into a mathematical vector (basically, a pair of numbers). The Neural Network of the AI is built on the relationships between all these tokens, which attempts to mathematically reproduce Human language.</p>
<h2 id="token-budget">Token Budget</h2>
<p><em>Text Injection</em></p>
<p>How many tokens are allowed to be used at all by the entry. Generally this is always set to the max window size, and priority takes care of the rest.</p>
<h2 id="token-id">Token ID</h2>
<p><em>AI</em></p>
<p>Remember how every token is a vector? Every vector has a number which serves as its identifying number. For example, [198] is the "new lin" character, so banning it prevents the AI from creating any new lines of its own.</p>
<h2 id="tokenization">Tokenization</h2>
<p><em>AI</em></p>
<p>Turning raw text into tokens. The AI is unable to read pure text data, and must turn it into tokens first.</p>
<h2 id="tokensafe">Tokensafe</h2>
<p><em>Writing, Technique</em></p>
<p>A technique to present information to the AI, a "format". It formats lorebook entries by using headers followed by singular adjectives or nominal groups separated by slashes.</p>
<h2 id="top-a">Top-A</h2>
<p><em>Sampling</em></p>
<p>A sampling method which adjusts the number of selected tokens based on the top token's probability. Lower probabilities mean more tokens.</p>
<h2 id="top-k">Top-K</h2>
<p><em>Sampling</em></p>
<p>A basic form of sampling that selects the K-most likely tokens. A top-K of 10 thus selects the top 10 most likely tokens.</p>
<h2 id="top-kek">Top-Kek</h2>
<p><em>Sampling</em></p>
<p>A sampling method developed by Kurumuz. Which is now known as Top-A.</p>
<h2 id="top-p">Top-P</h2>
<p><em>Sampling</em></p>
<p>Synonym of <em>Nucleus Sampling.</em> P stands for Probability.</p>
<h2 id="typical-sampling">Typical Sampling</h2>
<p><em>Sampling</em></p>
<p>An entropy-based approach to sampling, which samples based on deviation from a base line of entropy rather than probabilities.</p>
<h2 id="train-a-model">Train (a Model)</h2>
<p><em>AI</em></p>
<p>A GPT model is "pre-trained" on a large corpus of text files in order to create its token database, and analyze the relationships between the tokens. This takes a <strong>considerable</strong> amount of processing power and time, and is usually performed on very large, powerful servers or cloud servers.</p>
<h2 id="trim">Trim</h2>
<p><em>Text Injection, Feature</em></p>
<p><strong>Disambiguation:</strong></p>
<p>‚Ä¢ Trimming Entries: Any Text Injection can be trimmed down to the Newline, Sentence, or Token, if it is unable to fit in the context window in full.</p>
<p>‚Ä¢ Trim Trailing Spaces: Removes any space left at the very end of the story text, if it exists. Most tokens begin with a space, so leaving one can cause strange generations since most tokens aren't able to appear.</p>
<p>‚Ä¢ Trim AI responses: A deprecated feature that would cut off the AI generation at the latest sentence delimiter. Nowadays, this is "Continue Response to End of Sentence" in the Account Settings.</p>
<h2 id="undesired-content">Undesired Content</h2>
<p><em>Image Generation</em></p>
<p>Also known as a "Negative Prompt", very similar to negative Phrase Bias, but for image generation. It heavily weakens a tag, in order to make it less likely to appear.</p>
<!-- Javascript added by darkmode.lua to allow dark mode button -->
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
  function addDarkmodeWidget() {
    const darkmode_options = {
      mixColor: '#fff', // default: '#fff'
      backgroundColor: '#fff',  // default: '#fff'
      buttonColorDark: '#100f2c',  // default: '#100f2c'
      buttonColorLight: '#fff', // default: '#fff'
      label: 'üåì',
    }

    new Darkmode(darkmode_options).showWidget();
  }
  window.addEventListener('load', addDarkmodeWidget);
</script>


<!-- Javascript added by toc-css.lua to make TOC expandable on click -->
<script>

  const b = document.querySelector("body");
  const n = document.querySelector("nav");
  const buttonsize = 15;

  // click on "toc-title" to show TOC to the side
  document.querySelector("#toc-title").addEventListener("click", function(e) {
    if (e.clientX < e.currentTarget.getBoundingClientRect().left + buttonsize) {
      n.classList.toggle("navshown");
    } else {
      b.classList.toggle("paddingleft");
      n.classList.toggle("navside");
      n.classList.remove("navshown");
    };
  });

  // by default show TOC in large window
  window.onload = function() {
    if (window.innerWidth > 1000) {
      b.classList.add("paddingleft");
      n.classList.add("navside");
    };
  };

  // show/hide TOC on resize
  window.onresize = function () {
    // scrolling on mobile devices triggers resize, so we disable it altogether (for now)
    if (typeof window.orientation !== 'undefined') {
      return;
    };

    if (window.innerWidth > 1000) {
      b.classList.add("paddingleft");
      n.classList.add("navside");
    } else {
      b.classList.remove("paddingleft");
      n.classList.remove("navside");
    };
  };

  // show/hide subsections
  const allLis = document.querySelectorAll("nav li");

  for (const li of allLis) {
    li.addEventListener('click', function (e) {
      // show/hide subsection if arrow is clicked
      if (e.clientX < e.currentTarget.getBoundingClientRect().left + buttonsize
        && e.clientY < e.currentTarget.getBoundingClientRect().top + buttonsize) {

        li.classList.toggle('subShow');
        e.preventDefault();
      };

      // hide full nav if clicked outside
      if (e.currentTarget.getBoundingClientRect().left + 3*buttonsize < e.clientX) {
        n.classList.remove("navshown");
      };
    });

    li.classList.add('subShow');
  };

  // hide ToC if no overruling
  document.querySelector("nav ul").classList.add("toc-invisible");

  // show full nav on tab, hide full nav on escape
  document.addEventListener("keydown", function (e) {
    if (e.which === 27) {
      n.classList.remove("navshown");
      e.preventDefault();
    };
    if (e.which === 9) {
      n.classList.add("navshown");
      e.preventDefault();
    };
  });

  // insert key info
  n.insertAdjacentHTML("beforeend", "<span class='toc-invisible'> \
                                     <br/> \
                                     <p>Press <kbd>Tab</kbd> to show extended width TOC.</p> \
                                     <p>Presss <kbd>Esc</kbd> to go back to normal width.</p> \
                                     </span>");

  // hide full nav when clicked outside
  document.addEventListener("click", function(e) {
    if (n.classList.contains("navshown")) {
      if (!n.contains(e.target)) {
        n.classList.remove("navshown");
      };
    };
  });

</script>
</body>
</html>
